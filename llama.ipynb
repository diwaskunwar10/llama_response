{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pip 24.0 from /home/diwas/.local/lib/python3.10/site-packages/pip (python 3.10)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting llama-cpp-python==0.1.78\n",
      "  Downloading llama_cpp_python-0.1.78.tar.gz (1.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25h  Running command pip subprocess to install build dependencies\n",
      "  Collecting setuptools>=42\n",
      "    Using cached setuptools-69.1.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Collecting scikit-build>=0.13\n",
      "    Using cached scikit_build-0.17.6-py3-none-any.whl.metadata (14 kB)\n",
      "  Collecting cmake>=3.18\n",
      "    Using cached cmake-3.28.3-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "  Collecting ninja\n",
      "    Using cached ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
      "  Collecting distro (from scikit-build>=0.13)\n",
      "    Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "  Collecting packaging (from scikit-build>=0.13)\n",
      "    Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "  Collecting tomli (from scikit-build>=0.13)\n",
      "    Using cached tomli-2.0.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "  Collecting wheel>=0.32.0 (from scikit-build>=0.13)\n",
      "    Using cached wheel-0.42.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "  Using cached setuptools-69.1.1-py3-none-any.whl (819 kB)\n",
      "  Using cached scikit_build-0.17.6-py3-none-any.whl (84 kB)\n",
      "  Using cached cmake-3.28.3-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.3 MB)\n",
      "  Using cached ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "  Using cached wheel-0.42.0-py3-none-any.whl (65 kB)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "  Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "  Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "  Installing collected packages: ninja, cmake, wheel, tomli, setuptools, packaging, distro, scikit-build\n",
      "  Successfully installed cmake-3.28.3 distro-1.9.0 ninja-1.11.1.1 packaging-23.2 scikit-build-0.17.6 setuptools-69.1.1 tomli-2.0.1 wheel-0.42.0\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Running command Getting requirements to build wheel\n",
      "  running egg_info\n",
      "  writing llama_cpp_python.egg-info/PKG-INFO\n",
      "  writing dependency_links to llama_cpp_python.egg-info/dependency_links.txt\n",
      "  writing requirements to llama_cpp_python.egg-info/requires.txt\n",
      "  writing top-level names to llama_cpp_python.egg-info/top_level.txt\n",
      "  reading manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
      "  adding license file 'LICENSE.md'\n",
      "  writing manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Running command Preparing metadata (pyproject.toml)\n",
      "  running dist_info\n",
      "  creating /tmp/pip-modern-metadata-vbi_tir3/llama_cpp_python.egg-info\n",
      "  writing /tmp/pip-modern-metadata-vbi_tir3/llama_cpp_python.egg-info/PKG-INFO\n",
      "  writing dependency_links to /tmp/pip-modern-metadata-vbi_tir3/llama_cpp_python.egg-info/dependency_links.txt\n",
      "  writing requirements to /tmp/pip-modern-metadata-vbi_tir3/llama_cpp_python.egg-info/requires.txt\n",
      "  writing top-level names to /tmp/pip-modern-metadata-vbi_tir3/llama_cpp_python.egg-info/top_level.txt\n",
      "  writing manifest file '/tmp/pip-modern-metadata-vbi_tir3/llama_cpp_python.egg-info/SOURCES.txt'\n",
      "  reading manifest file '/tmp/pip-modern-metadata-vbi_tir3/llama_cpp_python.egg-info/SOURCES.txt'\n",
      "  adding license file 'LICENSE.md'\n",
      "  writing manifest file '/tmp/pip-modern-metadata-vbi_tir3/llama_cpp_python.egg-info/SOURCES.txt'\n",
      "  creating '/tmp/pip-modern-metadata-vbi_tir3/llama_cpp_python-0.1.78.dist-info'\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting numpy==1.23.4\n",
      "  Obtaining dependency information for numpy==1.23.4 from https://files.pythonhosted.org/packages/0c/83/78ae18fffc185d0d57097610d5a97473ef11dbdca95f16739ee96b158087/numpy-1.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading numpy-1.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
      "Collecting typing-extensions>=4.5.0 (from llama-cpp-python==0.1.78)\n",
      "  Obtaining dependency information for typing-extensions>=4.5.0 from https://files.pythonhosted.org/packages/f9/de/dc04a3ea60b22624b51c703a84bbe0184abcd1d0b9bc8074b5d6b7ab90bb/typing_extensions-4.10.0-py3-none-any.whl.metadata\n",
      "  Downloading typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.1.78)\n",
      "  Obtaining dependency information for diskcache>=5.6.1 from https://files.pythonhosted.org/packages/3f/27/4570e78fc0bf5ea0ca45eb1de3818a23787af9b390c0b0a0033a1b8236f9/diskcache-5.6.3-py3-none-any.whl.metadata\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Downloading numpy-1.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m150.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Running command Building wheel for llama-cpp-python (pyproject.toml)\n",
      "\n",
      "\n",
      "  --------------------------------------------------------------------------------\n",
      "  -- Trying 'Ninja' generator\n",
      "  --------------------------------\n",
      "  ---------------------------\n",
      "  ----------------------\n",
      "  -----------------\n",
      "  ------------\n",
      "  -------\n",
      "  --\n",
      "  \u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
      "    Compatibility with CMake < 3.5 will be removed from a future version of\n",
      "    CMake.\n",
      "\n",
      "    Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
      "    CMake that the project does not need compatibility with older versions.\n",
      "\n",
      "  \u001b[0mNot searching for unused variables given on the command line.\n",
      "\n",
      "  -- The C compiler identification is GNU 11.4.0\n",
      "  -- Detecting C compiler ABI info\n",
      "  -- Detecting C compiler ABI info - done\n",
      "  -- Check for working C compiler: /usr/bin/cc - skipped\n",
      "  -- Detecting C compile features\n",
      "  -- Detecting C compile features - done\n",
      "  -- The CXX compiler identification is GNU 11.4.0\n",
      "  -- Detecting CXX compiler ABI info\n",
      "  -- Detecting CXX compiler ABI info - done\n",
      "  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "  -- Detecting CXX compile features\n",
      "  -- Detecting CXX compile features - done\n",
      "  -- Configuring done (0.6s)\n",
      "  -- Generating done (0.0s)\n",
      "  -- Build files have been written to: /tmp/pip-install-d177tj7y/llama-cpp-python_ab1b45ca053b43e68117b84e0ce1e23a/_cmake_test_compile/build\n",
      "  --\n",
      "  -------\n",
      "  ------------\n",
      "  -----------------\n",
      "  ----------------------\n",
      "  ---------------------------\n",
      "  --------------------------------\n",
      "  -- Trying 'Ninja' generator - success\n",
      "  --------------------------------------------------------------------------------\n",
      "\n",
      "  Configuring Project\n",
      "    Working directory:\n",
      "      /tmp/pip-install-d177tj7y/llama-cpp-python_ab1b45ca053b43e68117b84e0ce1e23a/_skbuild/linux-x86_64-3.10/cmake-build\n",
      "    Command:\n",
      "      /tmp/pip-build-env-w2wv7ehh/overlay/local/lib/python3.10/dist-packages/cmake/data/bin/cmake /tmp/pip-install-d177tj7y/llama-cpp-python_ab1b45ca053b43e68117b84e0ce1e23a -G Ninja -DCMAKE_MAKE_PROGRAM:FILEPATH=/tmp/pip-build-env-w2wv7ehh/overlay/local/lib/python3.10/dist-packages/ninja/data/bin/ninja --no-warn-unused-cli -DCMAKE_INSTALL_PREFIX:PATH=/tmp/pip-install-d177tj7y/llama-cpp-python_ab1b45ca053b43e68117b84e0ce1e23a/_skbuild/linux-x86_64-3.10/cmake-install -DPYTHON_VERSION_STRING:STRING=3.10.12 -DSKBUILD:INTERNAL=TRUE -DCMAKE_MODULE_PATH:PATH=/tmp/pip-build-env-w2wv7ehh/overlay/local/lib/python3.10/dist-packages/skbuild/resources/cmake -DPYTHON_EXECUTABLE:PATH=/usr/bin/python3 -DPYTHON_INCLUDE_DIR:PATH=/usr/include/python3.10 -DPYTHON_LIBRARY:PATH=/usr/lib/x86_64-linux-gnu/libpython3.10.so -DPython_EXECUTABLE:PATH=/usr/bin/python3 -DPython_ROOT_DIR:PATH=/usr -DPython_FIND_REGISTRY:STRING=NEVER -DPython_INCLUDE_DIR:PATH=/usr/include/python3.10 -DPython3_EXECUTABLE:PATH=/usr/bin/python3 -DPython3_ROOT_DIR:PATH=/usr -DPython3_FIND_REGISTRY:STRING=NEVER -DPython3_INCLUDE_DIR:PATH=/usr/include/python3.10 -DCMAKE_MAKE_PROGRAM:FILEPATH=/tmp/pip-build-env-w2wv7ehh/overlay/local/lib/python3.10/dist-packages/ninja/data/bin/ninja -DLLAMA_CUBLAS=on -DCMAKE_BUILD_TYPE:STRING=Release -DLLAMA_CUBLAS=on\n",
      "\n",
      "  Not searching for unused variables given on the command line.\n",
      "  -- The C compiler identification is GNU 11.4.0\n",
      "  -- The CXX compiler identification is GNU 11.4.0\n",
      "  -- Detecting C compiler ABI info\n",
      "  -- Detecting C compiler ABI info - done\n",
      "  -- Check for working C compiler: /usr/bin/cc - skipped\n",
      "  -- Detecting C compile features\n",
      "  -- Detecting C compile features - done\n",
      "  -- Detecting CXX compiler ABI info\n",
      "  -- Detecting CXX compiler ABI info - done\n",
      "  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "  -- Detecting CXX compile features\n",
      "  -- Detecting CXX compile features - done\n",
      "  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
      "  fatal: not a git repository (or any of the parent directories): .git\n",
      "  fatal: not a git repository (or any of the parent directories): .git\n",
      "  \u001b[33mCMake Warning at vendor/llama.cpp/CMakeLists.txt:117 (message):\n",
      "    Git repository not found; to enable automatic generation of build info,\n",
      "    make sure Git is installed and the project is a Git repository.\n",
      "\n",
      "  \u001b[0m\n",
      "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "  -- Found Threads: TRUE\n",
      "  -- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version \"12.3.107\")\n",
      "  -- cuBLAS found\n",
      "  -- The CUDA compiler identification is unknown\n",
      "  \u001b[31mCMake Error at /tmp/pip-build-env-w2wv7ehh/overlay/local/lib/python3.10/dist-packages/cmake/data/share/cmake-3.28/Modules/CMakeDetermineCUDACompiler.cmake:270 (message):\n",
      "    Failed to detect a default CUDA architecture.\n",
      "\n",
      "\n",
      "\n",
      "    Compiler output:\n",
      "\n",
      "  Call Stack (most recent call first):\n",
      "    vendor/llama.cpp/CMakeLists.txt:250 (enable_language)\n",
      "\n",
      "  \u001b[0m\n",
      "  -- Configuring incomplete, errors occurred!\n",
      "  Traceback (most recent call last):\n",
      "    File \"/tmp/pip-build-env-w2wv7ehh/overlay/local/lib/python3.10/dist-packages/skbuild/setuptools_wrap.py\", line 666, in setup\n",
      "      env = cmkr.configure(\n",
      "    File \"/tmp/pip-build-env-w2wv7ehh/overlay/local/lib/python3.10/dist-packages/skbuild/cmaker.py\", line 357, in configure\n",
      "      raise SKBuildError(msg)\n",
      "\n",
      "  An error occurred while configuring with CMake.\n",
      "    Command:\n",
      "      /tmp/pip-build-env-w2wv7ehh/overlay/local/lib/python3.10/dist-packages/cmake/data/bin/cmake /tmp/pip-install-d177tj7y/llama-cpp-python_ab1b45ca053b43e68117b84e0ce1e23a -G Ninja -DCMAKE_MAKE_PROGRAM:FILEPATH=/tmp/pip-build-env-w2wv7ehh/overlay/local/lib/python3.10/dist-packages/ninja/data/bin/ninja --no-warn-unused-cli -DCMAKE_INSTALL_PREFIX:PATH=/tmp/pip-install-d177tj7y/llama-cpp-python_ab1b45ca053b43e68117b84e0ce1e23a/_skbuild/linux-x86_64-3.10/cmake-install -DPYTHON_VERSION_STRING:STRING=3.10.12 -DSKBUILD:INTERNAL=TRUE -DCMAKE_MODULE_PATH:PATH=/tmp/pip-build-env-w2wv7ehh/overlay/local/lib/python3.10/dist-packages/skbuild/resources/cmake -DPYTHON_EXECUTABLE:PATH=/usr/bin/python3 -DPYTHON_INCLUDE_DIR:PATH=/usr/include/python3.10 -DPYTHON_LIBRARY:PATH=/usr/lib/x86_64-linux-gnu/libpython3.10.so -DPython_EXECUTABLE:PATH=/usr/bin/python3 -DPython_ROOT_DIR:PATH=/usr -DPython_FIND_REGISTRY:STRING=NEVER -DPython_INCLUDE_DIR:PATH=/usr/include/python3.10 -DPython3_EXECUTABLE:PATH=/usr/bin/python3 -DPython3_ROOT_DIR:PATH=/usr -DPython3_FIND_REGISTRY:STRING=NEVER -DPython3_INCLUDE_DIR:PATH=/usr/include/python3.10 -DCMAKE_MAKE_PROGRAM:FILEPATH=/tmp/pip-build-env-w2wv7ehh/overlay/local/lib/python3.10/dist-packages/ninja/data/bin/ninja -DLLAMA_CUBLAS=on -DCMAKE_BUILD_TYPE:STRING=Release -DLLAMA_CUBLAS=on\n",
      "    Source directory:\n",
      "      /tmp/pip-install-d177tj7y/llama-cpp-python_ab1b45ca053b43e68117b84e0ce1e23a\n",
      "    Working directory:\n",
      "      /tmp/pip-install-d177tj7y/llama-cpp-python_ab1b45ca053b43e68117b84e0ce1e23a/_skbuild/linux-x86_64-3.10/cmake-build\n",
      "  Please see CMake's output for more information.\n",
      "\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for llama-cpp-python \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  \u001b[1;35mfull command\u001b[0m: \u001b[34m/usr/bin/python3 /home/diwas/.local/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py build_wheel /tmp/tmppsdtvjvf\u001b[0m\n",
      "  \u001b[1;35mcwd\u001b[0m: /tmp/pip-install-d177tj7y/llama-cpp-python_ab1b45ca053b43e68117b84e0ce1e23a\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
      "\u001b[31m  ERROR: Failed building wheel for llama-cpp-python\u001b[0m\u001b[31m\n",
      "\u001b[0mFailed to build llama-cpp-python\n",
      "\u001b[31mERROR: Could not build wheels for llama-cpp-python, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 numpy==1.23.4 --force-reinstall --upgrade --no-cache-dir --verbose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "\n",
    "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
    "model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /home/diwas/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 9 (mostly Q5_1)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
      "llama_model_load_internal: mem required  = 9311.07 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "llama_new_context_with_model: compute buffer total size =   75.35 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_path = \"~/cache/models--meta-llama--Llama-2-7b-hf/model.bin\" \n",
    "# Check if the model is already downloaded\n",
    "if not os.path.exists(model_path):\n",
    "    # If not downloaded, download the model\n",
    "    model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
    "\n",
    "# Initialize Llama model\n",
    "lcpp_llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_threads=2,       # Number of CPU cores\n",
    "    n_batch=512,       # Number of batches, consider your VRAM\n",
    "    n_gpu_layers=32     # Adjust based on your model and GPU VRAM\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "USER: Write 300 words essay about capital city of Nepal\n",
      "\n",
      "Llama_response:\n",
      "Here is a 300-word essay on the capital city of Nepal. Please note that this information may not be up to date, as I'm just an AI and do not have access to current events or recent changes in the city.\n",
      "\n",
      "The capital city of Nepal is Kathmandu. Located in the central region of the country, Kathmandu is a bustling metropolis with a rich history and culture dating back over 2,000 years. The city is home to numerous UNESCO World Heritage sites, including the famous Boudhanath Stupa and Pashupatinath Temple.\n",
      "\n",
      "One of the most striking features of Kathmandu is its unique blend of traditional architecture and modern infrastructure. The city's historic center is filled with narrow streets lined with brick-red buildings topped by golden pagoda-style roofs, while high-rise hotels and office buildings tower over the skyline.\n",
      "\n",
      "Kathmandu is also known for its vibrant arts and crafts scene. The city is home to numerous workshops and studios where local artisans create intricate woodcarvings, textiles, and other handicrafts using traditional techniques passed down through generations. Visitors can explore these workshops and purchase unique souvenirs to bring back home.\n",
      "\n",
      "In addition to its cultural attractions, Kathmandu offers a range of outdoor activities for visitors. The city is surrounded by four major mountains - Shivapuri, Phulchoki, Nagarjun, and Chandragiri - which offer hiking trails, rock climbing routes, and panoramic views of the city and surrounding valleys.\n",
      "\n",
      "Despite its many charms, Kathmandu is not without its challenges. The city struggles with overcrowding, pollution, and traffic congestion, which can make it difficult for visitors to navigate at times. However, with careful planning and a willingness to venture beyond the city's center, travelers can experience the true beauty of Kathmandu and its surroundings.\n",
      "\n",
      "In conclusion, Kathmandu is a unique and captivating destination that offers something for everyone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 13837.89 ms\n",
      "llama_print_timings:      sample time =   231.58 ms /   484 runs   (    0.48 ms per token,  2089.96 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13837.68 ms /    28 tokens (  494.20 ms per token,     2.02 tokens per second)\n",
      "llama_print_timings:        eval time = 217441.36 ms /   483 runs   (  450.19 ms per token,     2.22 tokens per second)\n",
      "llama_print_timings:       total time = 232515.19 ms\n"
     ]
    }
   ],
   "source": [
    "prompt = input(\"What do you want to ask\")\n",
    "\n",
    "prompt_template = f'''\n",
    "USER: {prompt}\n",
    "\n",
    "Llama_response:\n",
    "'''\n",
    "\n",
    "# Generate response\n",
    "response = lcpp_llm(prompt=prompt_template, max_tokens=2000, temperature=0.5, top_p=0.95,\n",
    "                    repeat_penalty=1.2, top_k=150,\n",
    "                    echo=True)\n",
    "\n",
    "\n",
    "print(response[\"choices\"][0][\"text\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
